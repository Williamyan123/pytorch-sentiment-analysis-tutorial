{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "First, as always, let's set the random seeds for deterministic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer for Sentiment Analysis Using BERT model\n",
    "#Import library\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is trained with the exact same vocab and tokenizer as the BERT pre-trained data.\n",
    "#Load the pre-trained `bert-base-uncased` tokenizer.\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "30522\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "#Check number of vocab\n",
    "print(len(tokenizer.vocab))\n",
    "#Check maximum length of input\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n",
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "#Test tokenizer to tokenize and lower case data\n",
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "print(tokens)\n",
    "#Numericalize tokens\n",
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n",
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "#Special tokens\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)\n",
    "\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that tokenizes text to input of the model\n",
    "#Note that the max length is 2 less than actual because we need to append \"Start\" and \"end\" tokens\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our fields. The transformer expects the batch dimension to be first, so we set `batch_first = True`. As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. We pass our `tokenize_and_cut` function as the tokenizer. The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. Finally, we define the special tokens - making note that we are defining them to be their index value and not their string value, i.e. `100` instead of `[UNK]` This is because the sequences will already be converted into indexes.\n",
    "\n",
    "We define the label field as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Fields\n",
    "from torchtext.legacy import data\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and create the validation splits as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [1996, 18458, 1997, 6644, 9016, 4627, 2066, 2009, 2453, 2031, 2242, 2000, 3749, 1012, 1037, 2177, 1997, 2267, 13496, 2044, 4399, 1006, 1999, 1996, 2991, 1029, 1007, 3632, 2000, 1037, 7001, 6644, 1999, 1996, 5249, 2073, 2028, 2011, 2028, 2027, 2024, 4457, 2011, 2019, 16100, 5771, 5983, 7865, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 6854, 1010, 1996, 2034, 20423, 2003, 2073, 2151, 6556, 3787, 1997, 2143, 3737, 2644, 1012, 6644, 9016, 2003, 2210, 2062, 2084, 2267, 4268, 2559, 2005, 3348, 1010, 22017, 4371, 1010, 3331, 2512, 1011, 2644, 2055, 2498, 1010, 1998, 3773, 2129, 2116, 1042, 1011, 9767, 2027, 2064, 2131, 2046, 1015, 1024, 2871, 2781, 2030, 2174, 2146, 2023, 6752, 2003, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 4268, 2552, 1998, 10509, 5236, 2135, 2000, 2673, 2105, 2068, 1012, 2028, 1997, 2068, 2005, 6013, 9418, 2008, 1996, 3096, 7865, 2038, 10372, 2014, 3456, 1010, 2061, 2054, 2515, 2016, 2079, 1029, 2016, 7906, 21146, 6455, 2014, 3456, 7989, 2000, 2202, 5372, 2966, 3086, 2005, 2014, 8710, 1012, 1996, 3496, 2003, 2210, 2062, 2084, 1037, 7977, 2041, 1012, 1999, 2178, 3496, 1010, 7945, 2844, 2013, 1000, 2879, 6010, 2088, 1000, 4152, 19026, 2006, 1996, 2192, 2011, 2070, 4845, 2040, 2069, 2758, 1000, 28470, 1000, 1998, 7777, 2000, 2079, 16894, 14590, 2006, 2216, 2040, 4133, 2279, 2000, 2032, 1012, 2065, 2017, 2064, 3275, 2041, 1996, 3114, 2005, 2339, 1996, 1000, 28470, 1000, 4845, 2001, 2443, 1010, 1045, 1005, 1040, 2293, 2000, 2113, 1012, 4312, 1010, 7945, 18551, 1037, 3748, 3899, 1998, 3632, 2125, 2000, 9378, 2010, 19026, 2192, 1999, 1037, 2087, 3497, 19450, 3636, 1012, 2178, 4845, 7777, 2000, 4530, 1042, 1011, 9767, 1999, 24868, 2000, 2673, 2105, 2032, 1998, 5607, 29384, 1012, 2339, 1029, 2115, 3984, 2003, 2004, 2204, 2004, 3067, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 7945, 2844, 2003, 1996, 2069, 4845, 2007, 2151, 5038, 1999, 2023, 3185, 1012, 2002, 5363, 2000, 5475, 2111, 2091, 1999, 1011, 2090, 1996, 13175, 1998, 7491, 1998, 1042, 1008, 1008, 1008, 1061, 1008, 1008, 999, 9767, 2008, 2111, 2024, 6886, 2105, 1012, 2043, 1996, 4268, 4995, 1005, 1056, 13175, 1010, 2027, 2024, 2383, 2030, 3331, 2055, 3348, 2030, 3331, 14652, 2000, 1996, 2060, 4639, 3494, 2040, 2024, 2130, 2062, 1006, 2065, 2008, 2003, 2825, 1007, 10041, 2594, 2084, 1996, 4268, 999, 1996, 10041, 8872, 2007, 2019, 26264, 1997, 3438, 2012, 2190, 2089, 2022, 2028, 1997, 1996, 5409, 3772, 5841, 1045, 2031, 2412, 2464, 1999, 1037, 3185, 1012, 2017, 2831, 2055, 2111, 2025, 2652, 2007, 1037, 2440, 5877, 1010, 2023, 2079, 8024, 2987, 1005, 1056, 2130, 2113, 2129, 2000, 2424, 1996, 5329, 999, 8840, 2140, 999, 1045, 2001, 2066, 1010, 1000, 2097, 2017, 3531, 3844, 2039, 2525, 1029, 999, 1000, 2002, 3084, 1996, 4845, 5889, 2298, 2066, 11067, 2229, 999, 1996, 2069, 2112, 2008, 1045, 4066, 1997, 4669, 2001, 7945, 1005, 1055, 12459, 2466, 1006, 2348, 2175, 2854, 1007, 2055, 1996, 4315, 22043, 2094, 9116, 8975, 3124, 1012, 1999, 7636, 1010, 7945, 2056, 2008, 2002, 2018, 1037, 2307, 3066, 1997, 4847, 2005, 2472, 12005, 12211, 1012], 'label': 'neg'}\n",
      "['the', 'premise', 'of', 'cabin', 'fever', 'starts', 'like', 'it', 'might', 'have', 'something', 'to', 'offer', '.', 'a', 'group', 'of', 'college', 'teens', 'after', 'finals', '(', 'in', 'the', 'fall', '?', ')', 'goes', 'to', 'a', 'resort', 'cabin', 'in', 'the', 'woods', 'where', 'one', 'by', 'one', 'they', 'are', 'attacked', 'by', 'an', 'unseen', 'flesh', 'eating', 'virus', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'unfortunately', ',', 'the', 'first', 'paragraph', 'is', 'where', 'any', 'remote', 'elements', 'of', 'film', 'quality', 'stop', '.', 'cabin', 'fever', 'is', 'little', 'more', 'than', 'college', 'kids', 'looking', 'for', 'sex', ',', 'boo', '##ze', ',', 'talking', 'non', '-', 'stop', 'about', 'nothing', ',', 'and', 'seeing', 'how', 'many', 'f', '-', 'bombs', 'they', 'can', 'get', 'into', '1', ':', '40', 'minutes', 'or', 'however', 'long', 'this', 'mess', 'is', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'kids', 'act', 'and', 'react', 'stupid', '##ly', 'to', 'everything', 'around', 'them', '.', 'one', 'of', 'them', 'for', 'instance', 'discovers', 'that', 'the', 'skin', 'virus', 'has', 'infected', 'her', 'legs', ',', 'so', 'what', 'does', 'she', 'do', '?', 'she', 'keeps', 'sha', '##ving', 'her', 'legs', 'failing', 'to', 'take', 'proper', 'medical', 'attention', 'for', 'her', 'wounds', '.', 'the', 'scene', 'is', 'little', 'more', 'than', 'a', 'gross', 'out', '.', 'in', 'another', 'scene', ',', 'rider', 'strong', 'from', '\"', 'boy', 'meets', 'world', '\"', 'gets', 'bitten', 'on', 'the', 'hand', 'by', 'some', 'kid', 'who', 'only', 'says', '\"', 'pancakes', '\"', 'and', 'likes', 'to', 'do', 'karate', 'kicks', 'on', 'those', 'who', 'sit', 'next', 'to', 'him', '.', 'if', 'you', 'can', 'figure', 'out', 'the', 'reason', 'for', 'why', 'the', '\"', 'pancakes', '\"', 'kid', 'was', 'included', ',', 'i', \"'\", 'd', 'love', 'to', 'know', '.', 'anyway', ',', 'rider', 'pets', 'a', 'wild', 'dog', 'and', 'goes', 'off', 'to', 'wash', 'his', 'bitten', 'hand', 'in', 'a', 'most', 'likely', 'contaminated', 'creek', '.', 'another', 'kid', 'likes', 'to', 'drop', 'f', '-', 'bombs', 'in', 'reacting', 'to', 'everything', 'around', 'him', 'and', 'shoot', 'squirrels', '.', 'why', '?', 'your', 'guess', 'is', 'as', 'good', 'as', 'mine', '!', '<', 'br', '/', '>', '<', 'br', '/', '>', 'rider', 'strong', 'is', 'the', 'only', 'kid', 'with', 'any', 'recognition', 'in', 'this', 'movie', '.', 'he', 'tries', 'to', 'calm', 'people', 'down', 'in', '-', 'between', 'the', 'yelling', 'and', 'screaming', 'and', 'f', '*', '*', '*', 'y', '*', '*', '!', 'bombs', 'that', 'people', 'are', 'throwing', 'around', '.', 'when', 'the', 'kids', 'aren', \"'\", 't', 'yelling', ',', 'they', 'are', 'having', 'or', 'talking', 'about', 'sex', 'or', 'talking', 'nonsense', 'to', 'the', 'other', 'adult', 'characters', 'who', 'are', 'even', 'more', '(', 'if', 'that', 'is', 'possible', ')', 'idiot', '##ic', 'than', 'the', 'kids', '!', 'the', 'idiot', 'cop', 'with', 'an', 'iq', 'of', '60', 'at', 'best', 'may', 'be', 'one', 'of', 'the', 'worst', 'acting', 'jobs', 'i', 'have', 'ever', 'seen', 'in', 'a', 'movie', '.', 'you', 'talk', 'about', 'people', 'not', 'playing', 'with', 'a', 'full', 'deck', ',', 'this', 'do', '##rk', 'doesn', \"'\", 't', 'even', 'know', 'how', 'to', 'find', 'the', 'cards', '!', 'lo', '##l', '!', 'i', 'was', 'like', ',', '\"', 'will', 'you', 'please', 'shut', 'up', 'already', '?', '!', '\"', 'he', 'makes', 'the', 'kid', 'actors', 'look', 'like', 'genius', '##es', '!', 'the', 'only', 'part', 'that', 'i', 'sort', 'of', 'liked', 'was', 'rider', \"'\", 's', 'scary', 'story', '(', 'although', 'go', '##ry', ')', 'about', 'the', 'der', '##ange', '##d', 'bowling', 'alley', 'guy', '.', 'in', 'interviews', ',', 'rider', 'said', 'that', 'he', 'had', 'a', 'great', 'deal', 'of', 'respect', 'for', 'director', 'eli', 'roth', '.']\n"
     ]
    }
   ],
   "source": [
    "#Check Example\n",
    "print(vars(train_data.examples[6]))\n",
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we've handled the vocabulary for the text, we still need to build the vocabulary for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "#Biuld labels\n",
    "LABEL.build_vocab(train_data)\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Iterator\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Next, we'll load the pre-trained model, making sure to load the same model as we did for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our actual model. \n",
    "\n",
    "Instead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n",
    "\n",
    "Within the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it. The rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an instance of our model using standard hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many parameters the model has. Our standard models have under 5M, but this one has 112M! Luckily, 110M of these parameters are from the transformer and we will not be training those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 112,241,409 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to freeze paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that our model has under 3M trainable parameters, making it almost comparable to the `FastText` model. However, the text still has to propagate through the transformer which causes training to take considerably longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,759,169 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check the names of the trainable parameters, ensuring they make sense. As we can see, they are all the parameters of the GRU (`rnn`) and the linear layer (`out`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "As is standard, we define our optimizer and criterion (loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place the model and criterion onto the GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll train our model. This takes considerably longer than any of the previous models due to the size of the transformer. Even though we are not training any of the transformer's parameters we still need to pass the data through the model which takes a considerable amount of time on a standard GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 20m 19s\n",
      "\tTrain Loss: 0.474 | Train Acc: 76.66%\n",
      "\t Val. Loss: 0.272 |  Val. Acc: 89.33%\n",
      "Epoch: 02 | Epoch Time: 20m 35s\n",
      "\tTrain Loss: 0.274 | Train Acc: 89.05%\n",
      "\t Val. Loss: 0.243 |  Val. Acc: 90.18%\n",
      "Epoch: 03 | Epoch Time: 16m 52s\n",
      "\tTrain Loss: 0.228 | Train Acc: 91.20%\n",
      "\t Val. Loss: 0.222 |  Val. Acc: 91.17%\n",
      "Epoch: 04 | Epoch Time: 15m 56s\n",
      "\tTrain Loss: 0.214 | Train Acc: 91.71%\n",
      "\t Val. Loss: 0.258 |  Val. Acc: 90.77%\n",
      "Epoch: 05 | Epoch Time: 15m 57s\n",
      "\tTrain Loss: 0.179 | Train Acc: 93.13%\n",
      "\t Val. Loss: 0.255 |  Val. Acc: 90.01%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load up the parameters that gave us the best validation loss and try these on the test set - which gives us our best results so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.206 | Test Acc: 91.77%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03066004440188408"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9059004187583923"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9951480031013489\n"
     ]
    }
   ],
   "source": [
    "comment=input()\n",
    "print(predict_sentiment(model,tokenizer,comment))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
